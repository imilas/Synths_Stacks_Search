{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactive plot to check DVN results\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.manifold import TSNE, Isomap, LocallyLinearEmbedding, SpectralEmbedding\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "import ipywidgets as widgets\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from ast import literal_eval\n",
    "import io\n",
    "from PIL import Image\n",
    "import librosa\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "import os, random\n",
    "\n",
    "#if you want torch for spectrograms\n",
    "# import torch\n",
    "# from feature_extraction import pytorch_utils as pu\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "n_neighbors = 10\n",
    "FREQ_BINS=30\n",
    "TIME_STEPS=20\n",
    "SR=44100\n",
    "\n",
    "def plotly_able_df(df,num_d=2):\n",
    "    \n",
    "    df[\"feats\"]=df[\"feats\"].apply(literal_eval)\n",
    "    X=df[\"feats\"]\n",
    "    #convert series of arrays into a numpy array\n",
    "    X=pd.DataFrame(X.to_list()).to_numpy()\n",
    "    X = min_max_scaler.fit_transform(X)\n",
    "    time_start = time.time()\n",
    "#     tsne = TSNE(n_components=num_d, random_state=0, perplexity=100, verbose=1)\n",
    "    tsne = Isomap(n_neighbors, n_components=num_d)\n",
    "#     tsne = LocallyLinearEmbedding(n_neighbors,n_components=num_d)\n",
    "#     tsne = SpectralEmbedding(n_neighbors=10,n_components?=num_d,random_state=1)\n",
    "    X_2d = tsne.fit_transform(X)\n",
    "    \n",
    "    if num_d==2:\n",
    "        df2=pd.concat([df[\"path\"],df[\"label\"],df[\"drum_type\"],pd.Series(X_2d[:,0]),pd.Series(X_2d[:,1])],axis=1)\n",
    "        df2.columns=[\"path\",\"label\",\"drum_type\",\"D1\",\"D2\"]\n",
    "    if num_d==3:\n",
    "        df2=pd.concat([df[\"path\"],df[\"label\"],df[\"drum_type\"],pd.Series(X_2d[:,0]),pd.Series(X_2d[:,1]),pd.Series(X_2d[:,2])],axis=1)\n",
    "        df2.columns=[\"path\",\"label\",\"drum_type\",\"D1\",\"D2\",\"D3\"]\n",
    "    \n",
    "    df2.label = df2.label.astype('str')\n",
    "    return df2\n",
    "\n",
    "class interactive_graph():\n",
    "    def hover_fn(self,trace, points, state):\n",
    "        if points.point_inds:\n",
    "            ind = points.point_inds[0]\n",
    "            drmName=trace.customdata[ind][0][:]\n",
    "            filename=os.getcwd()+\"/\"+drmName\n",
    "            with open(filename,'rb') as f:\n",
    "                audio_data = f.read()\n",
    "            self.aud.value=audio_data\n",
    "            self.hover_data.value = str(drmName)+\"\\n\"\n",
    "            self.audioImg.value=self.audDisplay(filename)\n",
    "            \n",
    "    def audDisplay(self,f):\n",
    "        #this got annoying because widget only accepts byte version of images\n",
    "        audio_array=librosa.load(f)\n",
    "        signals=audio_array[0]\n",
    "        \n",
    "        #with torch\n",
    "        try:\n",
    "            torch\n",
    "            nz=np.max((SR-signals.shape[0],0))\n",
    "            signals=np.concatenate([signals[0:SR],np.zeros(nz)]).astype(\"float32\")\n",
    "            sound={\"signal\":torch.tensor(signals),\"label\":'',\"path\":'',\"drum_type\":''}\n",
    "            trns=pu.specTrans(FREQ_BINS,time_steps=TIME_STEPS)\n",
    "            ft=trns(sound)[\"feats\"]\n",
    "            sf=ft.detach().numpy()[0]\n",
    "            print(sf)\n",
    "            #flip upside down\n",
    "            sf=sf[-1:0:-1][:]\n",
    "        except:\n",
    "            #no torch\n",
    "            signals = np.append(signals[0:SR], max(0,SR - len(signals)) * [0])\n",
    "            sf = librosa.feature.melspectrogram(y=signals, sr=SR, n_fft=2000, hop_length=400, power=1.0)\n",
    "\n",
    "        x=plt.imshow(sf)\n",
    "        #convert to bytes so can be set to widget data\n",
    "        buf = io.BytesIO()\n",
    "        x.figure.savefig(buf, format='png')\n",
    "        buf.seek(0)\n",
    "        bufD=buf.getvalue()\n",
    "        buf.close()\n",
    "        \n",
    "        return bufD\n",
    "    def __init__(self,df,grouping_by=\"label\",title=\"\",num_d=2,sym_size=10,color=px.colors.qualitative.Vivid):\n",
    "        if num_d==2:\n",
    "            p = px.scatter(df, x=\"D1\",y=\"D2\",symbol=\"drum_type\",color=grouping_by,hover_data=[\"path\"],color_discrete_sequence=color)\n",
    "        elif num_d==3:\n",
    "            p = px.scatter_3d(df, x=\"D1\",y=\"D2\",z=\"D3\",color=grouping_by,hover_data=[\"path\"],symbol=\"label\",color_discrete_sequence=color)\n",
    "        for trace in p.data:\n",
    "            trace.update(hoverinfo=\"none\",hovertemplate= '')\n",
    "        p.update_traces(marker=dict(size=sym_size,\n",
    "                              line=dict(width=0,\n",
    "                                        color='DarkSlateGrey')),\n",
    "                  selector=dict(mode='markers'))\n",
    "\n",
    "        self.hover_data = widgets.Textarea()  \n",
    "        #audio and img widgets\n",
    "        self.aud=widgets.Audio(autoplay=True,loop=False,embedding=True)\n",
    "        self.audioImg=widgets.Image(\n",
    "            value=b'',\n",
    "            format='png',\n",
    "            width='30%', \n",
    "        )\n",
    "        #####\n",
    "        layout = go.Layout(hovermode=False,)\n",
    "        self.fig  = go.FigureWidget(p)\n",
    "        self.fig.update_layout(scene = dict(\n",
    "                    camera=dict(eye=dict(x=-1, y=-1, z=0)),\n",
    "                    aspectmode=\"cube\",),\n",
    "                    margin=dict( r=0, l=0, b=0, t=0))\n",
    "                    \n",
    "        for f in self.fig.data:\n",
    "            f.on_hover(self.hover_fn)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "339636751a0c445cb1c7da50c997c865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'customdata': array([['feature_extraction/drum_dbs/./dk_data/tom_high/Roland Tr-9â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2b23ba9533b4c1299dad57fa051e402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Textarea(value=''), Audio(value=b'', loop='False'), Image(value=b'', width='30%')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df_raw = pd.read_csv(\"feature_extraction/csvs/best64_60.csv\")\n",
    "# df_raw[\"path\"] = \"feature_extraction/drum_dbs/\"+df_raw[\"path\"]\n",
    "# df = plotly_able_df(df_raw,num_d=2)\n",
    "ig=interactive_graph(df,grouping_by=\"drum_type\",title=\"8 dim embedding\",num_d=2,sym_size=10,color=px.colors.qualitative.Alphabet,)\n",
    "display(ig.fig,widgets.HBox([ig.hover_data,ig.aud,ig.audioImg]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(\"feature_extraction/csvs/best64_60.csv\")\n",
    "df_raw[\"path\"] = \"feature_extraction/drum_dbs/\"+df_raw[\"path\"]\n",
    "df_3d = plotly_able_df(df_raw, num_d=3)\n",
    "ig=interactive_graph(df_3d,grouping_by=\"drum_type\",title=\"3d embedding\",num_d=3,sym_size=5,color=px.colors.qualitative.G10_r,)\n",
    "display(ig.fig,widgets.HBox([ig.hover_data,ig.aud,ig.audioImg]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
