{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data base from ./drum_dbs/radar ./drum_dbs/radar.dill\n",
      "loading data base from ./drum_dbs/sample_swap ./drum_dbs/sample_swap.dill\n",
      "loading data base from ./drum_dbs/noise ./drum_dbs/noise.dill\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4886, 1000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import imp\n",
    "import torchaudio\n",
    "import torchvision as tv\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "import os, random\n",
    "import pandas as pd\n",
    "import mir_utils as miru\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import librosa\n",
    "import pytorch_utils\n",
    "import pytorch_models\n",
    "#reload these libraries because I change them often-ish\n",
    "imp.reload(pytorch_utils)\n",
    "imp.reload(miru)\n",
    "imp.reload(pytorch_models)\n",
    "SR=44100\n",
    "#functions\n",
    "spec=torchaudio.functional.spectrogram\n",
    "\n",
    "db_name=\"sample_swap_free_final\"\n",
    "drum_df=pd.concat([miru.audioFrames(db_name=\"radar\",loadCache=True),miru.audioFrames(db_name=\"sample_swap\",loadCache=True)])\n",
    "noise_df=miru.audioFrames(db_name=\"noise\",loadCache=True)\n",
    "\n",
    "def getMeanLength(x):\n",
    "    gl=x.apply(lambda z: len(z[\"audio\"]),axis=1)\n",
    "    print(gl.mean()/SR,gl.mean(),x[\"label\"].iloc[0])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "noise_df=noise_df[0:1000]\n",
    "len(drum_df),len(noise_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "clap                401\n",
      "electronic_hits     235\n",
      "hat                1275\n",
      "kick               1334\n",
      "noise               168\n",
      "rim                  82\n",
      "shake               116\n",
      "snare              1035\n",
      "tom                 167\n",
      "voc                  73\n",
      "Name: path, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dLabels, dUniques=pd.factorize(drum_df[\"label\"].tolist())\n",
    "ndLabels, ndUniques=pd.factorize(noise_df[\"label\"].tolist())\n",
    "\n",
    "drum_df.loc[:,\"label_num\"]=dLabels\n",
    "noise_df.loc[:,\"label_num\"]=ndLabels+len(dUniques)\n",
    "\n",
    "allU=np.concatenate([dUniques,ndUniques])\n",
    "\n",
    "y=drum_df.groupby([\"label\"]).agg(\"count\")[\"path\"]\n",
    "print(y)\n",
    "weights=torch.tensor([1000/w for w in y.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a dataset\n",
    "class audioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,audio_frame,root_dir, task=\"keep_all\",transform=None):\n",
    "        self.root_dir=root_dir\n",
    "        self.task=task\n",
    "        self.audio_frame=audio_frame\n",
    "        self.transform = transform\n",
    "        self.minLength=SR\n",
    "#         self.minLength=SR//4\n",
    "        self.frame_pruning()\n",
    "    def __len__(self):\n",
    "        return len(self.audio_frame)\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        rows=self.audio_frame.iloc[idx]\n",
    "\n",
    "        signals,labels=rows[\"audio\"].tolist()[0:SR],rows[\"label_num\"].tolist()\n",
    "        signals,labels=torch.tensor(signals),torch.tensor(labels)\n",
    "        \n",
    "        nz=np.max((self.minLength-signals.shape[0],0))\n",
    "        signals=torch.cat([signals[0:self.minLength],torch.zeros(nz)],dim=0)\n",
    "\n",
    "        sound={\"signal\":signals,\"label\":labels,\"path\":rows[\"path\"],\"drum_type\":rows[\"label\"]}\n",
    "        \n",
    "        if self.transform:\n",
    "            sound = self.transform(sound)\n",
    "\n",
    "        return sound\n",
    "    \n",
    "    def frame_pruning(self):\n",
    "        #drum vs not drum classification:\n",
    "        if self.task==\"dvn\":\n",
    "            self.audio_frame.loc[self.audio_frame[\"label\"]!=\"synth_noise\",\"label_num\"]=0 \n",
    "            self.audio_frame.loc[self.audio_frame[\"label\"]==\"synth_noise\",\"label_num\"]=1 \n",
    "        #drum type classification\n",
    "        if self.task==\"dvd\":\n",
    "            self.audio_frame=self.audio_frame.loc[self.audio_frame[\"label\"]!=\"synth_noise\"]\n",
    "        if self.task==\"keep_all\":\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "all_frames=pd.concat([drum_df,noise_df],sort=True).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FREQ_BINS=30\n",
    "TIME_STEPS=20\n",
    "#defining transformations\n",
    "\n",
    "class specTrans(object):\n",
    "    def __init__(self,num_mels=50,SR=SR,time_steps=20,amp_to_power=False):\n",
    "        self.amp_to_power=amp_to_power\n",
    "        self.num_mels=num_mels\n",
    "        self.ampP=torchaudio.transforms.AmplitudeToDB(stype='power',top_db=10)\n",
    "        self.melP=torchaudio.transforms.MelScale(n_mels=self.num_mels, sample_rate=SR,n_stft=None)\n",
    "        self.hop_step=time_steps-1\n",
    "    def __call__(self, sample):\n",
    "        \n",
    "        wf,label,p,drum_type=sample[\"signal\"],sample[\"label\"],sample[\"path\"],sample[\"drum_type\"]\n",
    "        wf=wf.reshape(-1,len(wf))\n",
    "        sample_length=SR\n",
    "\n",
    "        num_bins=wf[0].shape[0]\n",
    "        win_length=SR//17\n",
    "        hop_step=SR//self.hop_step\n",
    "        window=torch.tensor([1]*win_length)\n",
    "        s=spec(wf, 0, window, num_bins, hop_step, win_length,2,normalized=False)\n",
    "        s=self.melP(s)\n",
    "        if self.amp_to_power:\n",
    "            s=self.ampP(s)\n",
    "        s = s - s.min()\n",
    "        s = s/s.abs().max()\n",
    "\n",
    "        freq=s\n",
    "        freq[torch.isnan(freq)]=0\n",
    "        freq=freq\n",
    "        return (freq.detach(),sample[\"drum_type\"])\n",
    "    \n",
    "#Apply each of the above transforms on sample.\n",
    "fig = plt.figure(figsize=(20,4))\n",
    "\n",
    "\n",
    "#make a data point real quick\n",
    "ds=audioDataset(pd.concat([all_frames],sort=False),\"./\",task=\"dvn\",)\n",
    "sample = ds[np.random.randint(len(drum_df))]\n",
    "sample={\"signal\":sample[\"signal\"],\"label\":sample[\"label\"],\"path\":sample[\"path\"],\"drum_type\":sample[\"drum_type\"]}\n",
    "\n",
    "tfList=[specTrans(FREQ_BINS,time_steps=TIME_STEPS)]\n",
    "\n",
    "for i, tsfrm in enumerate(tfList):\n",
    "    transformed_sample = tsfrm(sample)\n",
    "    ax = plt.subplot(1, 3, i + 1)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    ft=transformed_sample[0][0]\n",
    "    sf=ft.detach().numpy()\n",
    "    ft=ft.detach().numpy()\n",
    "    plt.title(\"Spectrum Features\",fontsize=20)\n",
    "    librosa.display.specshow(sf,cmap='gray_r',)\n",
    "    plt.xlabel(\"Time Step\",fontsize=20)\n",
    "    plt.ylabel(\"Magnitude of Bin\",fontsize=20)\n",
    "\n",
    "#         plt.savefig(\"./plots/ff3.pdf\",bbox_inches = \"tight\")\n",
    "# plt.show()\n",
    "# print(lmap[transformed_sample[\"label\"].item()],transformed_sample[\"label\"],len(sample[\"signal\"]))\n",
    "\n",
    "\n",
    "Audio(sample[\"signal\"],rate=SR,autoplay=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hat'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#defining data loaders\n",
    "FREQ_BINS = 30\n",
    "TIME_STEPS = 30            \n",
    "pt=tv.transforms.Compose([specTrans(FREQ_BINS,time_steps=TIME_STEPS)])\n",
    "train = all_frames.sample(frac=0.70,random_state=420) \n",
    "test_and_valid = all_frames.drop(train.index)\n",
    "test = test_and_valid.sample(frac=0.70,random_state=420) \n",
    "valid = test_and_valid.drop(test.index)\n",
    "\n",
    "spec_data_train = audioDataset(train,\".\",\"dvd\", transform=pt)\n",
    "spec_data_test = audioDataset(test,\".\",task=\"dvd\", transform=pt)\n",
    "spec_data_valid = audioDataset(valid,\".\",task=\"dvd\", transform=pt)\n",
    "\n",
    "spec_data_valid[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x=next(iter(train_loader))\n",
    "# print(len(train_dataset),x[0].shape)\n",
    "\n",
    "# x=next(iter(test_loader))\n",
    "# print(len(test_dataset),x[0].shape)\n",
    "\n",
    "# x=next(iter(valid_loader))\n",
    "# print(len(valid_dataset),x[0].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# replicating prototypical networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import learn2learn as l2l\n",
    "from learn2learn.data.transforms import NWays, KShots, LoadData, RemapLabels\n",
    "\n",
    "\n",
    "def pairwise_distances_logits(a, b):\n",
    "    n = a.shape[0]\n",
    "    m = b.shape[0]\n",
    "    logits = -((a.unsqueeze(1).expand(n, m, -1) -\n",
    "                b.unsqueeze(0).expand(n, m, -1))**2).sum(dim=2)\n",
    "    return logits\n",
    "\n",
    "\n",
    "def accuracy(predictions, targets):\n",
    "    predictions = predictions.argmax(dim=1).view(targets.shape)\n",
    "    return (predictions == targets).sum().float() / targets.size(0)\n",
    "\n",
    "\n",
    "class Convnet(nn.Module):\n",
    "\n",
    "    def __init__(self, x_dim=1, hid_dim=32, z_dim=64):\n",
    "        super().__init__()\n",
    "        self.encoder = l2l.vision.models.ConvBase(output_size=z_dim,\n",
    "                                                  hidden=hid_dim,\n",
    "                                                  channels=x_dim)\n",
    "        self.out_channels = 1600\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "\n",
    "def fast_adapt(model, batch, ways, shot, query_num, metric=None, device=None):\n",
    "    if metric is None:\n",
    "        metric = pairwise_distances_logits\n",
    "    if device is None:\n",
    "        device = model.device()\n",
    "    data, labels = batch\n",
    "    data = data.to(device)\n",
    "    labels = labels.to(device)\n",
    "    n_items = shot * ways\n",
    "\n",
    "    # Sort data samples by labels\n",
    "    # TODO: Can this be replaced by ConsecutiveLabels ?\n",
    "    sort = torch.sort(labels)\n",
    "    data = data.squeeze(0)[sort.indices].squeeze(0)\n",
    "    labels = labels.squeeze(0)[sort.indices].squeeze(0)\n",
    "\n",
    "    # Compute support and query embeddings\n",
    "    embeddings = model(data)\n",
    "    support_indices = np.zeros(data.size(0), dtype=bool)\n",
    "    selection = np.arange(ways) * (shot + query_num)\n",
    "    for offset in range(shot):\n",
    "        support_indices[selection + offset] = True\n",
    "    query_indices = torch.from_numpy(~support_indices)\n",
    "    support_indices = torch.from_numpy(support_indices)\n",
    "    support = embeddings[support_indices]\n",
    "    support = support.reshape(ways, shot, -1).mean(dim=1)\n",
    "    query = embeddings[query_indices]\n",
    "    print(query)\n",
    "    print(labels)\n",
    "    labels = labels[query_indices].long()\n",
    "\n",
    "    logits = pairwise_distances_logits(query, support)\n",
    "    loss = F.cross_entropy(logits, labels)\n",
    "    acc = accuracy(logits, labels)\n",
    "    return loss, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu available\n"
     ]
    }
   ],
   "source": [
    "max_epoch = 10\n",
    "\n",
    "shot = 5\n",
    "train_way = 10\n",
    "train_query = 1\n",
    "\n",
    "test_shot = 5\n",
    "test_way = 10\n",
    "test_query = 1\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.device_count():\n",
    "    print(\"gpu available\")\n",
    "    torch.cuda.manual_seed(43)\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "model = Convnet()\n",
    "model.to(device)\n",
    "\n",
    "# path_data = '~/data'\n",
    "# train_dataset = l2l.vision.datasets.MiniImagenet(\n",
    "#     root=path_data, mode='train')\n",
    "# valid_dataset = l2l.vision.datasets.MiniImagenet(\n",
    "#     root=path_data, mode='validation')\n",
    "# test_dataset = l2l.vision.datasets.MiniImagenet(\n",
    "#     root=path_data, mode='test')\n",
    "train_dataset = audioDataset(train,\".\",\"keep_all\", transform=pt)\n",
    "test_dataset  = audioDataset(test,\".\",task=\"keep_all\", transform=pt)\n",
    "valid_dataset  = audioDataset(test,\".\",task=\"keep_all\", transform=pt)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = l2l.data.MetaDataset(train_dataset)\n",
    "train_transforms = [\n",
    "    NWays(train_dataset, train_way),\n",
    "    KShots(train_dataset, train_query +  shot),\n",
    "    LoadData(train_dataset),\n",
    "    RemapLabels(train_dataset),\n",
    "]\n",
    "train_tasks = l2l.data.TaskDataset(train_dataset, task_transforms=train_transforms,num_tasks=200)\n",
    "train_loader = DataLoader(train_tasks, pin_memory=True, shuffle=True)\n",
    "\n",
    "valid_dataset = l2l.data.MetaDataset(valid_dataset)\n",
    "valid_transforms = [\n",
    "    NWays(valid_dataset,  test_way),\n",
    "    KShots(valid_dataset,  test_query +  test_shot),\n",
    "    LoadData(valid_dataset),\n",
    "    RemapLabels(valid_dataset),\n",
    "]\n",
    "valid_tasks = l2l.data.TaskDataset(valid_dataset,\n",
    "                                   task_transforms=valid_transforms,\n",
    "                                   num_tasks=50)\n",
    "valid_loader = DataLoader(valid_tasks, pin_memory=True, shuffle=True)\n",
    "\n",
    "test_dataset = l2l.data.MetaDataset(test_dataset)\n",
    "test_transforms = [\n",
    "    NWays(test_dataset,  test_way),\n",
    "    KShots(test_dataset,  test_query +  test_shot),\n",
    "    LoadData(test_dataset),\n",
    "    RemapLabels(test_dataset),\n",
    "]\n",
    "test_tasks = l2l.data.TaskDataset(test_dataset,\n",
    "                                  task_transforms=test_transforms,\n",
    "                                  num_tasks=100)\n",
    "test_loader = DataLoader(test_tasks, pin_memory=True, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=20, gamma=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, train, loss=0.8778 acc=0.7020\n",
      "epoch 1, val, loss=1.2874 acc=0.5840\n",
      "epoch 2, train, loss=0.7345 acc=0.7470\n",
      "epoch 2, val, loss=1.2710 acc=0.6240\n",
      "epoch 3, train, loss=0.6041 acc=0.7840\n",
      "epoch 3, val, loss=1.2297 acc=0.6120\n",
      "epoch 4, train, loss=0.6502 acc=0.7840\n",
      "epoch 4, val, loss=1.3862 acc=0.5860\n",
      "epoch 5, train, loss=0.5750 acc=0.8080\n",
      "epoch 5, val, loss=1.3416 acc=0.6080\n",
      "epoch 6, train, loss=0.4625 acc=0.8350\n",
      "epoch 6, val, loss=1.4514 acc=0.6140\n",
      "epoch 7, train, loss=0.4952 acc=0.8420\n",
      "epoch 7, val, loss=1.2800 acc=0.6060\n",
      "epoch 8, train, loss=0.4487 acc=0.8570\n",
      "epoch 8, val, loss=1.5088 acc=0.6000\n",
      "epoch 9, train, loss=0.3592 acc=0.8860\n",
      "epoch 9, val, loss=1.3270 acc=0.5960\n",
      "epoch 10, train, loss=0.3001 acc=0.9070\n",
      "epoch 10, val, loss=1.6608 acc=0.6080\n",
      "batch 1: 70.00(70.00)\n",
      "batch 2: 70.00(70.00)\n",
      "batch 3: 63.33(50.00)\n",
      "batch 4: 65.00(70.00)\n",
      "batch 5: 66.00(70.00)\n",
      "batch 6: 65.00(60.00)\n",
      "batch 7: 62.86(50.00)\n",
      "batch 8: 61.25(50.00)\n",
      "batch 9: 62.22(70.00)\n",
      "batch 10: 62.00(60.00)\n",
      "batch 11: 63.64(80.00)\n",
      "batch 12: 65.00(80.00)\n",
      "batch 13: 63.08(40.00)\n",
      "batch 14: 62.86(60.00)\n",
      "batch 15: 63.33(70.00)\n",
      "batch 16: 65.00(90.00)\n",
      "batch 17: 64.12(50.00)\n",
      "batch 18: 62.22(30.00)\n",
      "batch 19: 61.58(50.00)\n",
      "batch 20: 61.00(50.00)\n",
      "batch 21: 60.95(60.00)\n",
      "batch 22: 60.00(40.00)\n",
      "batch 23: 60.00(60.00)\n",
      "batch 24: 60.42(70.00)\n",
      "batch 25: 61.20(80.00)\n",
      "batch 26: 60.38(40.00)\n",
      "batch 27: 61.11(80.00)\n",
      "batch 28: 61.43(70.00)\n",
      "batch 29: 61.03(50.00)\n",
      "batch 30: 60.00(30.00)\n",
      "batch 31: 60.00(60.00)\n",
      "batch 32: 60.62(80.00)\n",
      "batch 33: 60.61(60.00)\n",
      "batch 34: 60.29(50.00)\n",
      "batch 35: 60.00(50.00)\n",
      "batch 36: 60.56(80.00)\n",
      "batch 37: 60.54(60.00)\n",
      "batch 38: 61.05(80.00)\n",
      "batch 39: 61.28(70.00)\n",
      "batch 40: 61.75(80.00)\n",
      "batch 41: 61.46(50.00)\n",
      "batch 42: 61.19(50.00)\n",
      "batch 43: 61.63(80.00)\n",
      "batch 44: 61.36(50.00)\n",
      "batch 45: 61.33(60.00)\n",
      "batch 46: 61.74(80.00)\n",
      "batch 47: 61.70(60.00)\n",
      "batch 48: 61.46(50.00)\n",
      "batch 49: 61.43(60.00)\n",
      "batch 50: 61.60(70.00)\n",
      "batch 51: 61.37(50.00)\n",
      "batch 52: 61.35(60.00)\n",
      "batch 53: 61.13(50.00)\n",
      "batch 54: 60.93(50.00)\n",
      "batch 55: 60.91(60.00)\n",
      "batch 56: 61.07(70.00)\n",
      "batch 57: 60.88(50.00)\n",
      "batch 58: 60.86(60.00)\n",
      "batch 59: 61.19(80.00)\n",
      "batch 60: 61.33(70.00)\n",
      "batch 61: 61.15(50.00)\n",
      "batch 62: 61.13(60.00)\n",
      "batch 63: 60.79(40.00)\n",
      "batch 64: 60.94(70.00)\n",
      "batch 65: 60.92(60.00)\n",
      "batch 66: 61.21(80.00)\n",
      "batch 67: 61.49(80.00)\n",
      "batch 68: 61.47(60.00)\n",
      "batch 69: 61.74(80.00)\n",
      "batch 70: 62.00(80.00)\n",
      "batch 71: 62.11(70.00)\n",
      "batch 72: 62.08(60.00)\n",
      "batch 73: 62.05(60.00)\n",
      "batch 74: 62.03(60.00)\n",
      "batch 75: 62.13(70.00)\n",
      "batch 76: 61.84(40.00)\n",
      "batch 77: 61.69(50.00)\n",
      "batch 78: 61.67(60.00)\n",
      "batch 79: 61.39(40.00)\n",
      "batch 80: 61.50(70.00)\n",
      "batch 81: 61.48(60.00)\n",
      "batch 82: 61.59(70.00)\n",
      "batch 83: 61.69(70.00)\n",
      "batch 84: 61.79(70.00)\n",
      "batch 85: 61.88(70.00)\n",
      "batch 86: 61.86(60.00)\n",
      "batch 87: 61.72(50.00)\n",
      "batch 88: 61.59(50.00)\n",
      "batch 89: 61.46(50.00)\n",
      "batch 90: 61.44(60.00)\n",
      "batch 91: 61.43(60.00)\n",
      "batch 92: 61.41(60.00)\n",
      "batch 93: 61.61(80.00)\n",
      "batch 94: 61.49(50.00)\n",
      "batch 95: 61.58(70.00)\n",
      "batch 96: 61.46(50.00)\n",
      "batch 97: 61.55(70.00)\n",
      "batch 98: 61.73(80.00)\n",
      "batch 99: 61.72(60.00)\n",
      "batch 100: 61.50(40.00)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1,  max_epoch + 1):\n",
    "    model.train()\n",
    "\n",
    "    loss_ctr = 0\n",
    "    n_loss = 0\n",
    "    n_acc = 0\n",
    "\n",
    "    for i in range(100):\n",
    "        batch = next(iter(train_loader))\n",
    "\n",
    "        loss, acc = fast_adapt(model,\n",
    "                               batch,\n",
    "                                train_way,\n",
    "                                shot,\n",
    "                                train_query,\n",
    "                               metric=pairwise_distances_logits,\n",
    "                               device=device)\n",
    "\n",
    "        loss_ctr += 1\n",
    "        n_loss += loss.item()\n",
    "        n_acc += acc\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    print('epoch {}, train, loss={:.4f} acc={:.4f}'.format(\n",
    "        epoch, n_loss/loss_ctr, n_acc/loss_ctr))\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    loss_ctr = 0\n",
    "    n_loss = 0\n",
    "    n_acc = 0\n",
    "    for i, batch in enumerate(valid_loader):\n",
    "        loss, acc = fast_adapt(model,\n",
    "                               batch,\n",
    "                                test_way,\n",
    "                                test_shot,\n",
    "                                test_query,\n",
    "                               metric=pairwise_distances_logits,\n",
    "                               device=device)\n",
    "\n",
    "        loss_ctr += 1\n",
    "        n_loss += loss.item()\n",
    "        n_acc += acc\n",
    "\n",
    "    print('epoch {}, val, loss={:.4f} acc={:.4f}'.format(\n",
    "        epoch, n_loss/loss_ctr, n_acc/loss_ctr))\n",
    "\n",
    "loss_ctr = 0\n",
    "n_acc = 0\n",
    "\n",
    "for i, batch in enumerate(test_loader, 1):\n",
    "    loss, acc = fast_adapt(model,\n",
    "                           batch,\n",
    "                            test_way,\n",
    "                            test_shot,\n",
    "                            test_query,\n",
    "                           metric=pairwise_distances_logits,\n",
    "                           device=device)\n",
    "    loss_ctr += 1\n",
    "    n_acc += acc\n",
    "    print('batch {}: {:.2f}({:.2f})'.format(\n",
    "        i, n_acc/loss_ctr * 100, acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            ...\n",
    "            }, PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
