{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import interpolate \n",
    "from torch.autograd import Variable\n",
    "import imp\n",
    "import torchaudio\n",
    "import torchvision as tv\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "import os, random\n",
    "import pandas as pd\n",
    "import mir_utils as miru\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import librosa\n",
    "import pytorch_utils\n",
    "import pytorch_models\n",
    "#reload these libraries because I change them often-ish\n",
    "imp.reload(pytorch_utils)\n",
    "imp.reload(miru)\n",
    "imp.reload(pytorch_models)\n",
    "from scipy.signal import resample\n",
    "from sklearn import preprocessing\n",
    "le_major = preprocessing.LabelEncoder()\n",
    "\n",
    "SR = 22050//2\n",
    "#functions\n",
    "spec = torchaudio.functional.spectrogram\n",
    "\n",
    "def getMeanLength(x):\n",
    "    gl=x.apply(lambda z: len(z[\"audio\"]),axis=1)\n",
    "    print(gl.mean()/SR,gl.mean(),x[\"label\"].iloc[0])\n",
    "\n",
    "audio_df = pd.read_csv(\"../csvs/audio_df.csv\")\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_df = audio_df[audio_df[\"maj\"]==\"drums\"]\n",
    "audio_df[\"min\"] = audio_df[\"min\"].str.replace(\"tom_high\",\"kick\")\n",
    "audio_df[\"min\"] = audio_df[\"min\"].str.replace(\"tom\",\"kick\")\n",
    "audio_df[\"min\"] = audio_df[\"min\"].str.replace(\"rim\",\"snare\")\n",
    "audio_df[\"min\"] = audio_df[\"min\"].str.replace(\"clap\",\"snare\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['hat', 'kick', 'shake', 'snare'], dtype=object),\n",
       " array(['hat', 'kick', 'hat', 'hat'], dtype=object))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le_major.fit(audio_df[\"min\"])\n",
    "le_major.classes_,le_major.inverse_transform([0,1,0,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.3862,  3.3441, 48.9174,  2.5961], device='cuda:1',\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define weights\n",
    "w = [len(audio_df)/np.sum(audio_df[\"min\"]==x) for x in le_major.classes_]\n",
    "w = torch.tensor(w).to(device)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.22 s, sys: 647 Âµs, total: 1.22 s\n",
      "Wall time: 1.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#define a dataset\n",
    "class audioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,audio_df,RSR=SR,transform=None):\n",
    "        self.audio_df = audio_df\n",
    "        self.minLength = SR\n",
    "    def __len__(self):\n",
    "        return len(self.audio_df)\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        row = self.audio_df.iloc[idx]\n",
    "        try:\n",
    "            signal,sr = librosa.load(row[\"path\"])\n",
    "        except:\n",
    "            signal = np.zeros(self.minLength)\n",
    "            sr = self.minLength\n",
    "        # resample to global SR\n",
    "        signal = librosa.resample(signal,sr,self.minLength)\n",
    "        # pad the audio length if too short\n",
    "        nz = np.max((self.minLength-signal.shape[0],0))\n",
    "        signal = np.concatenate([signal[0:self.minLength],np.zeros(nz)])\n",
    "        sound={\"signal\":signal,\"major\":row[\"maj\"],\"minor\":row[\"min\"],\"path\":row[\"path\"],\"sr\":SR}\n",
    "        return sound\n",
    "    \n",
    "data_train = audioDataset(audio_df,SR)\n",
    "train_loader = DataLoader(data_train, batch_size=32,shuffle=True, num_workers=0)\n",
    "d = next(iter(train_loader))\n",
    "# d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 11025])\n",
      "torch.Size([5512])\n",
      "shake\n",
      "torch.Size([1, 30, 9])\n",
      "5512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asalimi/miniconda3/lib/python3.7/site-packages/torch/functional.py:516: UserWarning: stft will require the return_complex parameter be explicitly  specified in a future PyTorch release. Use return_complex=False  to preserve the current behavior or return_complex=True to return  a complex output. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:653.)\n",
      "  normalized, onesided, return_complex)\n",
      "/home/asalimi/miniconda3/lib/python3.7/site-packages/torch/functional.py:516: UserWarning: The function torch.rfft is deprecated and will be removed in a future PyTorch release. Use the new torch.fft module functions, instead, by importing torch.fft and calling torch.fft.fft or torch.fft.rfft. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:590.)\n",
      "  normalized, onesided, return_complex)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "FREQ_BINS=30\n",
    "TIME_STEPS=10\n",
    "RESAMPLE_SR = 8000\n",
    "#defining transformations\n",
    "\n",
    "class specTrans(object):\n",
    "    def __init__(self,num_mels=50,sr=SR,time_steps=20,amp_to_power=True):\n",
    "        self.sr = sr\n",
    "        self.amp_to_power=amp_to_power\n",
    "        self.num_mels=num_mels\n",
    "        self.ampP=torchaudio.transforms.AmplitudeToDB(stype='power',top_db=60)\n",
    "        self.melP=torchaudio.transforms.MelScale(n_mels=self.num_mels, sample_rate=sr,n_stft=None)\n",
    "        self.hop_step=time_steps-1\n",
    "    def __call__(self, sample):\n",
    "        \n",
    "        wf=sample\n",
    "        wf=wf.reshape(-1,len(wf))\n",
    "        sample_length=self.sr\n",
    "\n",
    "        num_bins=wf[0].shape[0]\n",
    "        win_length=self.sr//17\n",
    "        hop_step=self.sr//self.hop_step\n",
    "        window=torch.tensor([1]*win_length)\n",
    "        s=spec(wf, 0, window, num_bins, hop_step, win_length,2,normalized=True)\n",
    "        s=self.melP(s)\n",
    "        if self.amp_to_power:\n",
    "            s=self.ampP(s)\n",
    "        s = s - s.min()\n",
    "        s = s/s.abs().max()\n",
    "\n",
    "        freq=s\n",
    "        freq[torch.isnan(freq)]=0\n",
    "        freq=freq\n",
    "        return freq.detach()\n",
    "    \n",
    "    \n",
    "\n",
    "SCALE_FACTOR = 0.5\n",
    "i = np.random.randint(len(d[\"signal\"]))\n",
    "\n",
    "original = d[\"signal\"]\n",
    "print(original.shape)\n",
    "resampled_d = interpolate(d[\"signal\"].reshape([len(d[\"signal\"]),1,-1]),scale_factor = SCALE_FACTOR,recompute_scale_factor=False).reshape([len(d[\"signal\"]),-1])\n",
    "signal = resampled_d[i]\n",
    "label = d[\"minor\"][i]\n",
    "fig = plt.figure(figsize=(20,4))\n",
    "spec_tf = specTrans(FREQ_BINS,time_steps=TIME_STEPS,sr=SR,amp_to_power=True)\n",
    "print(signal.shape)\n",
    "transformed_sample = spec_tf(original[i].float())\n",
    "ax = plt.subplot(1, 3, 1)\n",
    "plt.tight_layout()\n",
    "\n",
    "ft=transformed_sample[0]\n",
    "sf=ft.detach().numpy()\n",
    "plt.title(\"Spectrum Features\",fontsize=20)\n",
    "librosa.display.specshow(sf,cmap='gray_r',)\n",
    "plt.xlabel(\"Time Step\",fontsize=20)\n",
    "plt.ylabel(\"Magnitude of Bin\",fontsize=20)\n",
    "\n",
    "\n",
    "print(label)\n",
    "print(transformed_sample.shape)\n",
    "print(len(signal))\n",
    "Audio(signal[0:5000],rate=SR*0.5,autoplay=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 30, 9])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z =  torch.stack(list(map(spec_tf,d[\"signal\"].float())),0)[i]\n",
    "librosa.display.specshow(z[0].numpy(),cmap='gray_r',)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "      \n",
    "spec_tf = specTrans(FREQ_BINS,time_steps=TIME_STEPS,amp_to_power=True)\n",
    "\n",
    "adf = audio_df.copy()\n",
    "train = adf.sample(frac=0.95,random_state=420) \n",
    "test_and_valid = adf.drop(train.index)\n",
    "test = test_and_valid.sample(frac=0.90,random_state=420) \n",
    "# valid = test_and_valid.drop(test.index)\n",
    "\n",
    "train_loader = DataLoader(audioDataset(train,SR), batch_size=128,shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(audioDataset(test,SR), batch_size=256,shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5623, 266)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train),len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_Spec_DVD(nn.Module):\n",
    "    def __init__(self,embed_only=False,dropout=0.075):\n",
    "        super(Conv_Spec_DVD, self).__init__()\n",
    "        self.embed_only = embed_only\n",
    "        self.dropout = dropout\n",
    "        self.conv_1d = nn.Sequential(\n",
    "                nn.Conv1d(1,128,500, stride=2, padding=5),\n",
    "                nn.BatchNorm1d(128),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv1d(128, 128, 250, stride=2, padding=4),\n",
    "                nn.BatchNorm1d(128),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv1d(128, 256, 100, stride=2, padding=3),\n",
    "                nn.BatchNorm1d(256),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv1d(256, 256, 100, stride=2, padding=2),\n",
    "                nn.MaxPool1d(kernel_size=3, stride=2, padding=1),\n",
    "                nn.BatchNorm1d(256),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv1d(256, 128, 80, stride=1, padding=3),\n",
    "                nn.BatchNorm1d(128),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv1d(128, 128, 40, stride=1, padding=3),\n",
    "                nn.MaxPool1d(kernel_size=3, stride=2, padding=1),\n",
    "                nn.BatchNorm1d(128),\n",
    "                nn.ReLU())\n",
    "        self.spectrogram_layer = nn.Sequential(\n",
    "                nn.Linear(30*9,64),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(self.dropout),\n",
    "                nn.Linear(64,32),\n",
    "                nn.Dropout(self.dropout),\n",
    "                nn.Linear(32,16),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(16,16),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(self.dropout),\n",
    "                )\n",
    "        self.l2 = nn.Sequential(\n",
    "                nn.Linear(128,64),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(self.dropout),\n",
    "                nn.Linear(64,32),\n",
    "                nn.Dropout(self.dropout),\n",
    "                nn.Linear(32,16),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(self.dropout),\n",
    "                )\n",
    "        self.l3 = nn.Sequential(\n",
    "                  nn.Linear(16+16,32),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Dropout(self.dropout),\n",
    "                  nn.Linear(32,16),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Linear(16,4),\n",
    "                )\n",
    "\n",
    "    def forward(self, x_sig,x_spec):\n",
    "        x_sig = x_sig.float()\n",
    "        bs = x_sig.shape[0]\n",
    "        bs_spec = x_spec.shape[0]\n",
    "        x_sig = x_sig.reshape(bs,1,-1).to(device)\n",
    "        x1_1d = self.conv_1d(x_sig)\n",
    "        flat_spec = x_spec.reshape([bs_spec,-1])\n",
    "        x1_fc = self.spectrogram_layer(flat_spec)\n",
    "        x1_1d = x1_1d.reshape(bs,-1)\n",
    "        x2 = self.l2(x1_1d)\n",
    "        x_agg = torch.cat((x2,x1_fc),dim=1)\n",
    "        x_final = self.l3(x_agg)\n",
    "        return x_final\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "try:\n",
    "    del cnet\n",
    "except:\n",
    "    pass\n",
    "cnet = Conv_Spec_DVD(embed_only=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"models/1d_conv_dvd/0.542_0.1510_.checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(np.log(len(ecg_paths)/np.sum(ecg_paths).to_numpy()))).to(device)\n",
    "loss_func = nn.CrossEntropyLoss(weight = w.float() )\n",
    "lr = 0.001 # learning rate\n",
    "optimizer = torch.optim.Adam(cnet.parameters(), lr=lr)\n",
    "smallest_loss,smallest_vloss = 1,1\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0] val loss: 2.81350, loss: 1.37624\n",
      "[1, 5] val loss: 2.81861, loss: 1.37173\n",
      "[1, 10] val loss: 2.68911, loss: 1.35766\n",
      "[1, 15] val loss: 2.69621, loss: 1.32126\n",
      "[1, 20] val loss: 2.48660, loss: 1.29793\n",
      "[1, 25] val loss: 2.75829, loss: 1.17364\n",
      "[1, 30] val loss: 2.29730, loss: 1.17024\n",
      "[2, 0] val loss: 1.90972, loss: 1.03793\n",
      "[2, 5] val loss: 2.92596, loss: 1.11160\n",
      "[2, 10] val loss: 2.18394, loss: 1.05956\n",
      "[2, 15] val loss: 2.00158, loss: 0.95324\n",
      "[2, 20] val loss: 1.97463, loss: 1.06879\n",
      "[2, 25] val loss: 2.09656, loss: 0.82585\n",
      "[2, 30] val loss: 2.07094, loss: 0.77507\n",
      "[2, 35] val loss: 1.91943, loss: 0.92593\n",
      "[2, 40] val loss: 1.99271, loss: 0.86414\n",
      "[3, 0] val loss: 1.75030, loss: 0.61526\n",
      "[3, 5] val loss: 1.96307, loss: 0.87493\n",
      "[3, 10] val loss: 1.83830, loss: 0.75659\n",
      "[3, 15] val loss: 1.70240, loss: 0.82629\n",
      "[3, 20] val loss: 2.28679, loss: 0.72691\n",
      "[3, 25] val loss: 1.34982, loss: 0.74381\n",
      "[3, 30] val loss: 1.72242, loss: 0.77447\n",
      "[3, 35] val loss: 1.75981, loss: 0.79708\n",
      "[3, 40] val loss: 2.25095, loss: 0.68758\n",
      "[4, 0] val loss: 2.19791, loss: 0.71260\n",
      "[4, 5] val loss: 1.68340, loss: 0.72431\n",
      "[4, 10] val loss: 2.34904, loss: 0.72125\n",
      "[4, 15] val loss: 1.53901, loss: 0.69931\n",
      "[4, 20] val loss: 1.35498, loss: 0.59260\n",
      "[4, 25] val loss: 1.19873, loss: 0.80191\n",
      "[4, 30] val loss: 1.20489, loss: 0.61178\n",
      "[4, 35] val loss: 0.97394, loss: 0.74394\n",
      "[4, 40] val loss: 1.36434, loss: 0.69267\n",
      "[5, 0] val loss: 1.25786, loss: 0.68317\n",
      "[5, 5] val loss: 1.07605, loss: 0.51056\n",
      "[5, 10] val loss: 1.13555, loss: 0.61838\n",
      "[5, 15] val loss: 1.28761, loss: 0.60805\n",
      "[5, 20] val loss: 1.30838, loss: 0.60758\n",
      "[5, 25] val loss: 1.38562, loss: 0.63088\n",
      "[5, 30] val loss: 1.04168, loss: 0.60409\n",
      "[5, 35] val loss: 1.19035, loss: 0.70653\n",
      "[5, 40] val loss: 0.94156, loss: 0.51534\n",
      "[6, 0] val loss: 1.52930, loss: 0.63027\n",
      "[6, 5] val loss: 1.86243, loss: 0.46936\n",
      "[6, 10] val loss: 1.27656, loss: 0.51687\n",
      "[6, 15] val loss: 1.33536, loss: 0.51436\n",
      "[6, 20] val loss: 1.30754, loss: 0.82702\n",
      "[6, 25] val loss: 1.45406, loss: 0.62396\n",
      "[6, 30] val loss: 1.69533, loss: 0.53525\n",
      "[6, 35] val loss: 1.05542, loss: 0.56491\n",
      "[6, 40] val loss: 0.88718, loss: 0.72472\n",
      "[7, 0] val loss: 1.31500, loss: 0.56633\n",
      "[7, 5] val loss: 1.33527, loss: 0.61415\n",
      "[7, 10] val loss: 2.25332, loss: 0.57685\n",
      "[7, 15] val loss: 1.32508, loss: 0.62738\n",
      "[7, 20] val loss: 1.13439, loss: 0.61822\n",
      "[7, 25] val loss: 1.34124, loss: 0.64028\n",
      "[7, 30] val loss: 0.90133, loss: 0.55584\n",
      "[7, 35] val loss: 1.49059, loss: 0.46537\n",
      "[7, 40] val loss: 0.86492, loss: 0.57367\n",
      "[8, 0] val loss: 1.04336, loss: 0.57597\n",
      "[8, 5] val loss: 0.91865, loss: 0.54446\n",
      "[8, 10] val loss: 0.87959, loss: 0.57316\n",
      "[8, 15] val loss: 1.23741, loss: 0.57836\n",
      "[8, 20] val loss: 1.13000, loss: 0.58028\n",
      "[8, 25] val loss: 0.81739, loss: 0.49922\n",
      "[8, 30] val loss: 2.04022, loss: 0.44282\n",
      "[8, 35] val loss: 1.38084, loss: 0.55388\n",
      "[8, 40] val loss: 0.96943, loss: 0.53337\n",
      "[9, 0] val loss: 1.09234, loss: 0.43554\n",
      "[9, 5] val loss: 1.09090, loss: 0.42597\n",
      "[9, 10] val loss: 1.01044, loss: 0.52957\n",
      "[9, 15] val loss: 0.76241, loss: 0.46487\n",
      "[9, 20] val loss: 1.47959, loss: 0.53927\n",
      "[9, 25] val loss: 1.16760, loss: 0.53598\n",
      "[9, 30] val loss: 0.95725, loss: 0.51677\n",
      "[9, 35] val loss: 1.09025, loss: 0.43010\n",
      "[9, 40] val loss: 0.75312, loss: 0.51873\n",
      "[10, 0] val loss: 1.13224, loss: 0.52359\n",
      "[10, 5] val loss: 1.04328, loss: 0.52437\n",
      "[10, 10] val loss: 1.83697, loss: 0.44488\n",
      "[10, 15] val loss: 0.98948, loss: 0.44327\n",
      "[10, 20] val loss: 1.07618, loss: 0.52054\n",
      "[10, 25] val loss: 0.89461, loss: 0.34160\n",
      "[10, 30] val loss: 1.10945, loss: 0.44399\n",
      "[10, 35] val loss: 1.92909, loss: 0.58921\n",
      "[10, 40] val loss: 0.82914, loss: 0.46281\n",
      "[11, 0] val loss: 0.99851, loss: 0.50633\n",
      "[11, 5] val loss: 0.74816, loss: 0.47566\n",
      "[11, 10] val loss: 1.23624, loss: 0.45561\n",
      "[11, 15] val loss: 0.94347, loss: 0.48573\n",
      "[11, 20] val loss: 1.10320, loss: 0.39663\n",
      "[11, 25] val loss: 0.82331, loss: 0.59238\n",
      "[11, 30] val loss: 1.27560, loss: 0.53909\n",
      "[11, 35] val loss: 1.45747, loss: 0.68153\n",
      "[11, 40] val loss: 1.81973, loss: 0.50078\n",
      "[12, 0] val loss: 0.93864, loss: 0.51742\n",
      "[12, 5] val loss: 1.04994, loss: 0.50875\n",
      "[12, 10] val loss: 0.99532, loss: 0.52899\n",
      "[12, 15] val loss: 1.46861, loss: 0.40510\n",
      "[12, 20] val loss: 0.96515, loss: 0.33736\n",
      "[12, 25] val loss: 1.02492, loss: 0.36312\n",
      "[12, 30] val loss: 1.23163, loss: 0.43207\n",
      "[12, 35] val loss: 0.99737, loss: 0.46110\n",
      "[12, 40] val loss: 1.02450, loss: 0.52742\n",
      "[13, 0] val loss: 1.15681, loss: 0.44760\n",
      "[13, 5] val loss: 1.33182, loss: 0.59330\n",
      "[13, 10] val loss: 0.88760, loss: 0.34930\n",
      "[13, 15] val loss: 1.04206, loss: 0.72520\n",
      "[13, 20] val loss: 0.89630, loss: 0.53702\n",
      "[13, 25] val loss: 1.01458, loss: 0.49205\n",
      "[13, 30] val loss: 1.11519, loss: 0.33802\n",
      "[13, 35] val loss: 0.92844, loss: 0.39965\n",
      "[13, 40] val loss: 0.98487, loss: 0.46904\n",
      "[14, 0] val loss: 1.52820, loss: 0.47721\n",
      "[14, 5] val loss: 0.91310, loss: 0.47805\n",
      "[14, 10] val loss: 1.24094, loss: 0.45348\n",
      "[14, 15] val loss: 2.42883, loss: 0.40960\n",
      "[14, 20] val loss: 0.82220, loss: 0.32527\n",
      "[14, 25] val loss: 1.77614, loss: 0.33955\n",
      "[14, 30] val loss: 0.74469, loss: 0.70418\n",
      "[14, 35] val loss: 2.12284, loss: 0.43141\n",
      "[14, 40] val loss: 1.03319, loss: 0.36949\n",
      "[15, 0] val loss: 1.33072, loss: 0.46941\n",
      "[15, 5] val loss: 1.14816, loss: 0.41366\n",
      "[15, 10] val loss: 3.06492, loss: 0.55033\n",
      "[15, 15] val loss: 1.40800, loss: 0.53261\n",
      "[15, 20] val loss: 0.92277, loss: 0.51298\n",
      "[15, 25] val loss: 0.87493, loss: 0.64257\n",
      "[15, 30] val loss: 0.88644, loss: 0.53264\n",
      "[15, 35] val loss: 0.82585, loss: 0.43651\n",
      "[15, 40] val loss: 1.12420, loss: 0.35808\n",
      "[16, 0] val loss: 0.73679, loss: 0.40876\n",
      "[16, 5] val loss: 1.06799, loss: 0.46660\n",
      "[16, 10] val loss: 1.07048, loss: 0.41277\n",
      "[16, 15] val loss: 1.12912, loss: 0.45328\n",
      "[16, 20] val loss: 0.91260, loss: 0.62610\n",
      "[16, 25] val loss: 1.02771, loss: 0.47557\n",
      "[16, 30] val loss: 1.28094, loss: 0.41987\n",
      "[16, 35] val loss: 1.14613, loss: 0.43391\n",
      "[16, 40] val loss: 1.27399, loss: 0.50745\n",
      "[17, 0] val loss: 0.81486, loss: 0.47251\n",
      "[17, 5] val loss: 0.86946, loss: 0.40514\n",
      "[17, 10] val loss: 1.15899, loss: 0.35343\n",
      "[17, 15] val loss: 1.87433, loss: 0.60049\n",
      "[17, 20] val loss: 1.10897, loss: 0.33874\n",
      "[17, 25] val loss: 1.04615, loss: 0.37950\n",
      "[17, 30] val loss: 0.85027, loss: 0.32463\n",
      "[17, 35] val loss: 1.31486, loss: 0.30553\n",
      "[17, 40] val loss: 1.74250, loss: 0.27835\n",
      "[18, 0] val loss: 0.86267, loss: 0.56732\n",
      "[18, 5] val loss: 1.49166, loss: 0.47572\n",
      "[18, 10] val loss: 1.60590, loss: 0.38733\n",
      "[18, 15] val loss: 2.80293, loss: 0.43295\n",
      "[18, 20] val loss: 1.02702, loss: 0.35974\n",
      "[18, 25] val loss: 1.01764, loss: 0.48310\n",
      "[18, 30] val loss: 0.94544, loss: 0.39306\n",
      "[18, 35] val loss: 1.36332, loss: 0.53620\n",
      "[18, 40] val loss: 2.01891, loss: 0.38003\n",
      "[19, 0] val loss: 0.77243, loss: 0.40778\n",
      "[19, 5] val loss: 0.60257, loss: 0.45291\n",
      "[19, 10] val loss: 1.02571, loss: 0.46537\n",
      "[19, 15] val loss: 0.72916, loss: 0.40421\n",
      "[19, 20] val loss: 1.20755, loss: 0.26139\n",
      "[19, 25] val loss: 1.39202, loss: 0.50284\n",
      "[19, 30] val loss: 0.72728, loss: 0.28119\n",
      "[19, 35] val loss: 0.92136, loss: 0.33204\n",
      "[19, 40] val loss: 1.67676, loss: 0.37258\n",
      "[20, 0] val loss: 0.84422, loss: 0.26270\n",
      "[20, 5] val loss: 1.36568, loss: 0.25506\n",
      "[20, 10] val loss: 1.63536, loss: 0.23024\n",
      "[20, 15] val loss: 0.70702, loss: 0.39160\n",
      "[20, 20] val loss: 0.88065, loss: 0.33449\n",
      "[20, 25] val loss: 1.58868, loss: 0.38209\n",
      "[20, 30] val loss: 1.76033, loss: 0.20649\n",
      "[20, 35] val loss: 0.89492, loss: 0.59837\n",
      "[20, 40] val loss: 0.62514, loss: 0.43568\n",
      "[21, 0] val loss: 0.97631, loss: 0.29112\n",
      "[21, 5] val loss: 1.26024, loss: 0.32642\n",
      "[21, 10] val loss: 0.56530, loss: 0.33185\n",
      "[21, 15] val loss: 1.02075, loss: 0.33561\n",
      "[21, 20] val loss: 1.44398, loss: 0.28572\n",
      "[21, 25] val loss: 2.05960, loss: 0.43315\n",
      "[21, 30] val loss: 1.73826, loss: 0.59581\n",
      "[21, 35] val loss: 1.72166, loss: 0.36898\n",
      "[21, 40] val loss: 0.68726, loss: 0.40618\n",
      "[22, 0] val loss: 1.16092, loss: 0.35041\n",
      "[22, 5] val loss: 0.94558, loss: 0.26874\n",
      "[22, 10] val loss: 0.99437, loss: 0.52363\n",
      "[22, 15] val loss: 1.08174, loss: 0.35173\n",
      "[22, 20] val loss: 0.91954, loss: 0.31995\n",
      "[22, 25] val loss: 1.19882, loss: 0.27389\n",
      "[22, 30] val loss: 1.04015, loss: 0.49423\n",
      "[22, 35] val loss: 1.30905, loss: 0.36130\n",
      "[22, 40] val loss: 1.00640, loss: 0.44818\n",
      "[23, 0] val loss: 1.27346, loss: 0.35717\n",
      "[23, 5] val loss: 0.78708, loss: 0.48623\n",
      "[23, 10] val loss: 0.87415, loss: 0.22578\n",
      "[23, 15] val loss: 1.36647, loss: 0.23745\n",
      "[23, 20] val loss: 1.03283, loss: 0.33846\n",
      "[23, 25] val loss: 1.43501, loss: 0.44212\n",
      "[23, 30] val loss: 1.49796, loss: 0.33163\n",
      "[23, 35] val loss: 1.55636, loss: 0.24829\n",
      "[23, 40] val loss: 1.58511, loss: 0.25665\n",
      "[24, 0] val loss: 1.62100, loss: 0.47321\n",
      "[24, 5] val loss: 1.22043, loss: 0.35950\n",
      "[24, 10] val loss: 0.56962, loss: 0.25133\n",
      "[24, 15] val loss: 1.05116, loss: 0.23950\n",
      "[24, 20] val loss: 0.84667, loss: 0.18783\n",
      "[24, 25] val loss: 1.31546, loss: 0.24492\n",
      "[24, 30] val loss: 1.41728, loss: 0.25788\n",
      "[24, 35] val loss: 0.71217, loss: 0.25538\n",
      "[24, 40] val loss: 1.29087, loss: 0.33724\n",
      "[25, 0] val loss: 1.42490, loss: 0.28438\n",
      "[25, 5] val loss: 1.14746, loss: 0.58981\n",
      "[25, 10] val loss: 1.05208, loss: 0.26955\n",
      "[25, 15] val loss: 1.67130, loss: 0.24343\n",
      "[25, 20] val loss: 0.67773, loss: 0.35021\n",
      "[25, 25] val loss: 0.76885, loss: 0.23840\n",
      "[25, 30] val loss: 1.55607, loss: 0.17871\n",
      "[25, 35] val loss: 0.92796, loss: 0.31707\n",
      "[25, 40] val loss: 0.81682, loss: 0.32135\n",
      "[26, 0] val loss: 2.60529, loss: 0.22011\n",
      "[26, 5] val loss: 1.10537, loss: 0.20219\n",
      "[26, 10] val loss: 4.21977, loss: 0.22839\n",
      "[26, 15] val loss: 1.33842, loss: 0.14749\n",
      "[26, 20] val loss: 1.40689, loss: 0.18862\n",
      "[26, 25] val loss: 1.50862, loss: 0.51565\n",
      "[26, 30] val loss: 1.67728, loss: 0.19065\n",
      "[26, 35] val loss: 1.77640, loss: 0.37067\n",
      "[26, 40] val loss: 0.89509, loss: 0.29658\n",
      "[27, 0] val loss: 1.17259, loss: 0.23016\n",
      "[27, 5] val loss: 1.77296, loss: 0.23243\n",
      "[27, 10] val loss: 1.02994, loss: 0.19829\n",
      "[27, 15] val loss: 1.56109, loss: 0.18550\n",
      "[27, 20] val loss: 1.07843, loss: 0.18944\n",
      "[27, 25] val loss: 2.38698, loss: 0.30383\n",
      "[27, 30] val loss: 3.51641, loss: 0.34302\n",
      "[27, 35] val loss: 1.25239, loss: 0.50301\n",
      "[27, 40] val loss: 2.69192, loss: 0.30693\n",
      "[28, 0] val loss: 1.54097, loss: 0.32985\n",
      "[28, 5] val loss: 0.98540, loss: 0.18647\n",
      "[28, 10] val loss: 0.86731, loss: 0.20555\n",
      "[28, 15] val loss: 1.10719, loss: 0.25006\n",
      "[28, 20] val loss: 0.96867, loss: 0.27526\n",
      "[28, 25] val loss: 2.41584, loss: 0.19705\n",
      "[28, 30] val loss: 1.71474, loss: 0.24283\n",
      "[28, 35] val loss: 1.29274, loss: 0.22588\n",
      "[28, 40] val loss: 1.46798, loss: 0.20498\n",
      "[29, 0] val loss: 1.70209, loss: 0.22565\n",
      "[29, 5] val loss: 1.09725, loss: 0.18071\n",
      "[29, 10] val loss: 0.79043, loss: 0.28397\n",
      "[29, 15] val loss: 0.95972, loss: 0.20095\n",
      "[29, 20] val loss: 0.77095, loss: 0.23290\n",
      "[29, 25] val loss: 0.79651, loss: 0.21696\n",
      "[29, 30] val loss: 1.29927, loss: 0.27824\n",
      "[29, 35] val loss: 1.13077, loss: 0.22579\n",
      "[29, 40] val loss: 0.83948, loss: 0.25009\n",
      "[30, 0] val loss: 1.29175, loss: 0.14717\n",
      "[30, 5] val loss: 1.12508, loss: 0.19349\n",
      "[30, 10] val loss: 1.54479, loss: 0.29175\n",
      "[30, 15] val loss: 0.75817, loss: 0.18455\n",
      "[30, 20] val loss: 2.21125, loss: 0.29556\n",
      "[30, 25] val loss: 1.27354, loss: 0.37113\n",
      "[30, 30] val loss: 1.52523, loss: 0.24632\n",
      "[30, 35] val loss: 1.21876, loss: 0.40194\n",
      "[30, 40] val loss: 1.03489, loss: 0.28752\n",
      "[31, 0] val loss: 1.06040, loss: 0.37175\n",
      "[31, 5] val loss: 3.56318, loss: 0.36775\n",
      "[31, 10] val loss: 1.00981, loss: 0.31819\n",
      "[31, 15] val loss: 0.77593, loss: 0.26592\n",
      "[31, 20] val loss: 1.06742, loss: 0.31975\n",
      "[31, 25] val loss: 1.28048, loss: 0.32098\n",
      "[31, 30] val loss: 1.05221, loss: 0.38919\n",
      "[31, 35] val loss: 1.39748, loss: 0.15213\n",
      "[31, 40] val loss: 0.76786, loss: 0.31326\n",
      "[32, 0] val loss: 0.62723, loss: 0.28400\n",
      "[32, 5] val loss: 2.66620, loss: 0.30929\n",
      "[32, 10] val loss: 2.80263, loss: 0.34792\n",
      "[32, 15] val loss: 1.94786, loss: 0.40140\n",
      "[32, 20] val loss: 0.74514, loss: 0.25212\n",
      "[32, 25] val loss: 1.13964, loss: 0.38393\n",
      "[32, 30] val loss: 1.93162, loss: 0.24869\n",
      "[32, 35] val loss: 0.75538, loss: 0.23755\n",
      "[32, 40] val loss: 0.92639, loss: 0.22996\n",
      "[33, 0] val loss: 1.17963, loss: 0.12368\n",
      "[33, 5] val loss: 0.67242, loss: 0.25000\n",
      "[33, 10] val loss: 2.44488, loss: 0.21752\n",
      "[33, 15] val loss: 0.85765, loss: 0.44708\n",
      "[33, 20] val loss: 1.71860, loss: 0.26184\n",
      "[33, 25] val loss: 0.84606, loss: 0.23883\n",
      "[33, 30] val loss: 1.28061, loss: 0.61256\n",
      "[33, 35] val loss: 0.81139, loss: 0.26634\n",
      "[33, 40] val loss: 0.97377, loss: 0.27470\n",
      "[34, 0] val loss: 0.95567, loss: 0.30710\n",
      "[34, 5] val loss: 1.29448, loss: 0.25426\n",
      "[34, 10] val loss: 1.29918, loss: 0.12396\n",
      "[34, 15] val loss: 1.22373, loss: 0.18965\n",
      "[34, 20] val loss: 3.44810, loss: 0.17164\n",
      "[34, 25] val loss: 1.14352, loss: 0.20334\n",
      "[34, 30] val loss: 1.11318, loss: 0.15849\n",
      "[34, 35] val loss: 0.85869, loss: 0.21187\n",
      "[34, 40] val loss: 1.39564, loss: 0.18081\n",
      "[35, 0] val loss: 0.84960, loss: 0.14312\n",
      "[35, 5] val loss: 0.55900, loss: 0.12562\n",
      "[35, 10] val loss: 1.00341, loss: 0.15843\n",
      "[35, 15] val loss: 1.33368, loss: 0.23308\n",
      "[35, 20] val loss: 1.39138, loss: 0.11114\n",
      "[35, 25] val loss: 2.18072, loss: 0.23766\n",
      "[35, 30] val loss: 1.00526, loss: 0.50891\n",
      "[35, 35] val loss: 0.69430, loss: 0.33064\n",
      "[35, 40] val loss: 0.85083, loss: 0.18706\n",
      "[36, 0] val loss: 0.91601, loss: 0.28430\n",
      "[36, 5] val loss: 4.08603, loss: 0.15284\n",
      "[36, 10] val loss: 0.98797, loss: 0.19189\n",
      "[36, 15] val loss: 2.33494, loss: 0.19036\n",
      "[36, 20] val loss: 1.39141, loss: 0.17743\n",
      "[36, 25] val loss: 0.65060, loss: 0.24413\n",
      "[36, 30] val loss: 4.62916, loss: 0.15730\n",
      "[36, 35] val loss: 0.65964, loss: 0.23913\n",
      "[36, 40] val loss: 0.84366, loss: 0.21188\n",
      "[37, 0] val loss: 1.10296, loss: 0.15729\n",
      "[37, 5] val loss: 1.00090, loss: 0.24112\n",
      "[37, 10] val loss: 1.51811, loss: 0.17386\n",
      "[37, 15] val loss: 1.03335, loss: 0.18518\n",
      "[37, 20] val loss: 0.74106, loss: 0.28765\n",
      "[37, 25] val loss: 1.34714, loss: 0.19646\n",
      "[37, 30] val loss: 1.01733, loss: 0.32106\n",
      "[37, 35] val loss: 0.90200, loss: 0.27903\n",
      "[37, 40] val loss: 0.81596, loss: 0.16873\n",
      "[38, 0] val loss: 0.74304, loss: 0.31097\n",
      "[38, 5] val loss: 1.46046, loss: 0.14063\n",
      "[38, 10] val loss: 1.17562, loss: 0.20423\n",
      "[38, 15] val loss: 0.96497, loss: 0.31215\n",
      "[38, 20] val loss: 1.42152, loss: 0.09177\n",
      "[38, 25] val loss: 0.80119, loss: 0.20438\n",
      "[38, 30] val loss: 2.92992, loss: 0.13282\n",
      "[38, 35] val loss: 1.32518, loss: 0.29981\n",
      "[38, 40] val loss: 1.54307, loss: 0.18013\n",
      "[39, 0] val loss: 0.87174, loss: 0.19002\n",
      "[39, 5] val loss: 0.73786, loss: 0.27373\n",
      "[39, 10] val loss: 0.72807, loss: 0.15001\n",
      "[39, 15] val loss: 3.11806, loss: 0.09992\n",
      "[39, 20] val loss: 2.32226, loss: 0.19315\n",
      "[39, 25] val loss: 1.71924, loss: 0.16014\n",
      "[39, 30] val loss: 1.29751, loss: 0.11363\n",
      "[39, 35] val loss: 2.01182, loss: 0.20142\n",
      "[39, 40] val loss: 1.34971, loss: 0.17306\n",
      "[40, 0] val loss: 2.78075, loss: 0.13662\n",
      "[40, 5] val loss: 1.93317, loss: 0.12189\n",
      "[40, 10] val loss: 0.95723, loss: 0.12453\n",
      "[40, 15] val loss: 1.43979, loss: 0.08521\n",
      "[40, 20] val loss: 1.32253, loss: 0.43042\n",
      "[40, 25] val loss: 0.54747, loss: 0.12408\n",
      "[40, 30] val loss: 1.80857, loss: 0.19602\n",
      "[40, 35] val loss: 0.58374, loss: 0.28606\n",
      "[40, 40] val loss: 1.68663, loss: 0.09183\n"
     ]
    }
   ],
   "source": [
    "SCALE_FACTOR = 0.5\n",
    "num_windows = 5\n",
    "window_shift = 300\n",
    "smallest_loss,smallest_vloss = 1000,1000\n",
    "step = 0\n",
    "for epoch in range(40): \n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        train_loss = 0\n",
    "        bs = len(data[\"signal\"]) # batch size\n",
    "        signal = interpolate(data[\"signal\"].reshape([bs,1,-1]),scale_factor = SCALE_FACTOR,recompute_scale_factor=False).reshape([bs,1,-1])\n",
    "        spectrums = torch.stack(list(map(spec_tf,data[\"signal\"].float())),0).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnet(signal,spectrums)\n",
    "        y = torch.tensor(le_major.transform(data[\"minor\"])).to(device)\n",
    "        loss = loss_func(outputs,y.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        if i%5==0:\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0\n",
    "                for iv,datav in enumerate(test_loader, 0):\n",
    "                    signalv = interpolate(datav[\"signal\"].reshape([len(datav[\"signal\"]),1,-1]),scale_factor = SCALE_FACTOR,recompute_scale_factor=False).reshape([len(datav[\"signal\"]),-1])\n",
    "                    spectrumsv = torch.stack(list(map(spec_tf,datav[\"signal\"].float())),0).to(device)\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "                    voutputs = cnet(signalv,spectrumsv )\n",
    "                    vy = torch.tensor(le_major.transform(datav[\"minor\"])).to(device)\n",
    "                    vloss = loss_func(voutputs,vy.long())\n",
    "                    val_loss += vloss\n",
    "            print('[%d, %d] val loss: %.5f, loss: %.5f'%(epoch + 1, i , val_loss,train_loss))\n",
    "    #         writer.add_scalar('Loss/Training', train_loss,)\n",
    "    #         writer.add_scalar('Loss/Validation', val_loss)\n",
    "            if val_loss < smallest_vloss:        \n",
    "                torch.save({\n",
    "                'epoch': epoch,\n",
    "                'vloss': vloss,\n",
    "                'model_state_dict': cnet.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                }, \"models/1d_conv_dvd/%.3f_%.4f_.checkpoint\"%(val_loss,train_loss,))\n",
    "                smallest_vloss = val_loss\n",
    "                smallest_loss = train_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
