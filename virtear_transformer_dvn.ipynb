{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asalimi/miniconda3/lib/python3.7/site-packages/torchaudio/backend/utils.py:54: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  '\"sox\" backend is being deprecated. '\n",
      "/home/asalimi/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/home/asalimi/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/home/asalimi/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "/home/asalimi/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/home/asalimi/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/home/asalimi/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "/home/asalimi/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/home/asalimi/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/home/asalimi/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "/home/asalimi/miniconda3/lib/python3.7/site-packages/sklearn/decomposition/online_lda.py:29: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  EPS = np.finfo(np.float).eps\n",
      "/home/asalimi/miniconda3/lib/python3.7/site-packages/sklearn/feature_extraction/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import interpolate \n",
    "from torch.autograd import Variable\n",
    "import imp\n",
    "import torchaudio\n",
    "import torchvision as tv\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "import os, random\n",
    "import pandas as pd\n",
    "# import mir_utils as miru\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import librosa\n",
    "# import pytorch_utils\n",
    "# import pytorch_models\n",
    "#reload these libraries because I change them often-ish\n",
    "# imp.reload(pytorch_utils)\n",
    "# imp.reload(miru)\n",
    "# imp.reload(pytorch_models)\n",
    "from scipy.signal import resample\n",
    "from sklearn import preprocessing\n",
    "le_major = preprocessing.LabelEncoder()\n",
    "\n",
    "SR = 44100\n",
    "#functions\n",
    "spec = torchaudio.functional.spectrogram\n",
    "\n",
    "def getMeanLength(x):\n",
    "    gl=x.apply(lambda z: len(z[\"audio\"]),axis=1)\n",
    "    print(gl.mean()/SR,gl.mean(),x[\"label\"].iloc[0])\n",
    "\n",
    "audio_df = pd.read_csv(\"csvs/audio_df.csv\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_df[\"maj\"] = audio_df[\"maj\"].apply(lambda x: 1 if x==\"drums\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.76 s, sys: 9.14 ms, total: 1.77 s\n",
      "Wall time: 2.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#define a dataset\n",
    "class audioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,audio_df,SR=44100,transform=None):\n",
    "        self.audio_df = audio_df\n",
    "        self.minLength = SR//4\n",
    "    def __len__(self):\n",
    "        return len(self.audio_df)\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        row = self.audio_df.iloc[idx]\n",
    "        try:\n",
    "            signal,sr = librosa.load(row[\"path\"])\n",
    "        except:\n",
    "            signal = np.zeros(self.minLength)\n",
    "            sr = self.minLength\n",
    "        # resample to global SR\n",
    "        signal = librosa.resample(signal,sr,sr//2)\n",
    "        # pad the audio length if too short\n",
    "        nz = np.max((self.minLength-signal.shape[0],0))\n",
    "        signal = np.concatenate([signal[0:self.minLength],np.zeros(nz)])\n",
    "        \n",
    "        sound={\"signal\":signal,\"major\":row[\"maj\"],\"minor\":row[\"min\"],\"path\":row[\"path\"],\"sr\":SR}\n",
    "        return sound\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "adf = audio_df.copy().sample(frac=1)\n",
    "train,val = train_test_split(adf, test_size=0.1) \n",
    "\n",
    "train_loader = DataLoader(audioDataset(train,SR), batch_size=128,shuffle=False, num_workers=4)\n",
    "val_loader = DataLoader(audioDataset(val,SR), batch_size=128,shuffle=False, num_workers=4)\n",
    "d = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "338"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 11025])\n",
      "torch.Size([128, 1, 11025])\n",
      "torch.Size([128, 32, 352]) torch.Size([128, 32, 1]) torch.Size([128, 32, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torchmetrics\n",
    "from torchmetrics.functional import auc\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint,LearningRateMonitor\n",
    "\n",
    "class Transformer_Lightning(LightningModule):\n",
    "    def __init__(self,attention_dropout=0.5,d_model=128,heads=8,encoding_layers=8,pool_dim=1,pct_start=0.05,max_lr=1e-4,max_momentum=0.95,epochs = 50):\n",
    "        super().__init__()\n",
    "        dropout=0.2\n",
    "        self.attention_dropout=attention_dropout\n",
    "        self.d_model = d_model\n",
    "        self.heads = heads\n",
    "        self.encoding_layers = encoding_layers\n",
    "        self.pool_dim = pool_dim\n",
    "        self.pct_start = pct_start\n",
    "        self.max_lr = max_lr\n",
    "        self.max_momentum = max_momentum\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        self.conv_encoder = nn.Sequential(\n",
    "                    nn.Conv1d(1, self.d_model, kernel_size=10, stride=2, padding=5, bias=False),\n",
    "                    nn.BatchNorm1d(self.d_model),\n",
    "                    nn.MaxPool1d(kernel_size=3, stride=2, padding=1),\n",
    "                    \n",
    "                    nn.Conv1d(self.d_model, self.d_model, kernel_size=3, stride=2, padding=5, bias=False),\n",
    "                    nn.BatchNorm1d(self.d_model),\n",
    "                    nn.MaxPool1d(kernel_size=3, stride=2, padding=1),\n",
    "                    \n",
    "                    nn.Conv1d(self.d_model, self.d_model, kernel_size=3, stride=2, padding=7, bias=False),\n",
    "                    nn.BatchNorm1d(self.d_model),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    )\n",
    "        \n",
    "        self.pos_encoder = PositionalEncoding(self.d_model, 0.1)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=self.d_model, nhead=self.heads,\n",
    "                                                        dropout = self.attention_dropout,)\n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=self.encoding_layers)\n",
    "        self.adaptiveavgpool = nn.AdaptiveAvgPool1d(self.pool_dim)\n",
    "        self.adaptivemaxpool = nn.AdaptiveMaxPool1d(self.pool_dim)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "          nn.Linear(self.d_model*2*self.pool_dim,64),\n",
    "          nn.Linear(64,1),\n",
    "        )\n",
    "#         d[\"signal\"]\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        x = x.reshape([-1,1,SR//4]).float()\n",
    "        print(x.shape)\n",
    "        x1 = self.conv_encoder(x).transpose(2,1).transpose(1,0)\n",
    "        x1 = self.pos_encoder(x1)\n",
    "        x2 = self.transformer_encoder(x1).transpose(1,0).transpose(2,1)\n",
    "        x3r = self.adaptiveavgpool(x2)\n",
    "        x3c = self.adaptivemaxpool(x2)\n",
    "        print(x2.shape,x3r.shape,x3c.shape)\n",
    "        x4 = torch.cat((x3r, x3c), dim=1)\n",
    "        x4 = x4.view(x4.size(0), -1)\n",
    "        out =  self.decoder(x4)\n",
    "        return out\n",
    "    \n",
    "    def step(self, batch, batch_idx):\n",
    "        x, y = batch[\"signal\"].float(),batch[\"major\"].float().reshape(-1,1)\n",
    "        x = x.reshape([-1,1,SR//4]).float()\n",
    "        x1 = self.conv_encoder(x).transpose(2,1).transpose(1,0)\n",
    "        x1 = self.pos_encoder(x1)\n",
    "        x2 = self.transformer_encoder(x1).transpose(1,0).transpose(2,1)\n",
    "        x3r = self.adaptiveavgpool(x2)\n",
    "        x3c = self.adaptivemaxpool(x2)\n",
    "        x4 = torch.cat((x3r, x3c), dim=1)\n",
    "        x4 = x4.view(x4.size(0), -1)\n",
    "        out =  self.decoder(x4)\n",
    "#         loss = F.binary_cross_entropy_with_logits(out, y,pos_weight=self.w_pos.to(self.device))\n",
    "        loss = F.binary_cross_entropy_with_logits(out, y,)\n",
    "        auc = torchmetrics.functional.auroc(out,y.int(),num_classes=1,)\n",
    "        return loss, {\"loss\": loss,\"auc\":auc}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, logs = self.step(batch, batch_idx)\n",
    "\n",
    "        \n",
    "        self.log_dict({f\"train_{k}\": v for k, v in logs.items()}, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, logs = self.step(batch, batch_idx)\n",
    "        self.log_dict({f\"val_{k}\": v for k, v in logs.items()}, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-8)\n",
    "        lr_scheduler = {'scheduler': torch.optim.lr_scheduler.OneCycleLR(\n",
    "                                        optimizer,\n",
    "                                        pct_start = self.pct_start,\n",
    "                                        max_lr=self.max_lr,\n",
    "                                        steps_per_epoch=int(len(self.train_dataloader())),\n",
    "                                        epochs=self.epochs,\n",
    "                                        anneal_strategy=\"cos\",\n",
    "                                        final_div_factor = 1000,\n",
    "                                        max_momentum=self.max_momentum,\n",
    "                                    ),\n",
    "                        'name': 'learning_rate',\n",
    "                        'interval':'step',\n",
    "                        'frequency': 1}\n",
    "        return [optimizer],[lr_scheduler]\n",
    "\n",
    "    \n",
    "# model = Transformer_Lightning().load_from_checkpoint(\"models/transformer/epoch=15-val_loss=0.090-val_auc=0.876.ckpt\")\n",
    "model = Transformer_Lightning(attention_dropout=0.1,d_model=32,heads=8,encoding_layers=4,pool_dim=1)\n",
    "model(d[\"signal\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "338"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback =  ModelCheckpoint(\n",
    "    monitor='val_auc',\n",
    "    dirpath='models/transformer',\n",
    "    filename='{epoch:2d}-{val_loss:.3f}-{val_auc:.3f}',\n",
    "    save_top_k=5,\n",
    "    mode='max',\n",
    ")\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "trainer = pl.Trainer(gpus=1,precision=16,callbacks=[checkpoint_callback,lr_monitor],log_every_n_steps=1,max_epochs=100,)\n",
    "trainer.fit(model,train_loader,val_loader,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE_FACTOR = 0.5\n",
    "num_windows = 5\n",
    "window_shift = 300\n",
    "smallest_loss,smallest_vloss = 1000,1000\n",
    "step = 0\n",
    "for epoch in range(40): \n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        train_loss = 0\n",
    "        bs = len(data[\"signal\"]) # batch size\n",
    "        signal = interpolate(data[\"signal\"].reshape([bs,1,-1]),scale_factor = SCALE_FACTOR,recompute_scale_factor=False).reshape([bs,1,-1])\n",
    "      \n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnet(signal,)\n",
    "        y = torch.tensor(le_major.transform(data[\"major\"])).to(device)\n",
    "        loss = loss_func(outputs,y.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        if i%5==0:\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0\n",
    "                for iv,datav in enumerate(test_loader, 0):\n",
    "                    signalv = interpolate(datav[\"signal\"].reshape([len(datav[\"signal\"]),1,-1]),scale_factor = SCALE_FACTOR,recompute_scale_factor=False).reshape([len(datav[\"signal\"]),-1])\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "                    voutputs = cnet(signalv,)\n",
    "                    vy = torch.tensor(le_major.transform(datav[\"major\"])).to(device)\n",
    "                    vloss = loss_func(voutputs,vy.long())\n",
    "                    val_loss += vloss\n",
    "            print('[%d, %d] val loss: %.5f, loss: %.5f'%(epoch + 1, i , val_loss,train_loss))\n",
    "    #         writer.add_scalar('Loss/Training', train_loss,)\n",
    "    #         writer.add_scalar('Loss/Validation', val_loss)\n",
    "            if val_loss < smallest_vloss:        \n",
    "                torch.save({\n",
    "                'epoch': epoch,\n",
    "                'vloss': vloss,\n",
    "                'model_state_dict': cnet.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                }, \"models/1d_conv//%.3f_%.4f_.checkpoint\"%(val_loss,train_loss,))\n",
    "                smallest_vloss = val_loss\n",
    "                smallest_loss = train_loss "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
