{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import interpolate \n",
    "from torch.autograd import Variable\n",
    "import imp\n",
    "import torchaudio\n",
    "import torchvision as tv\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "import librosa.display\n",
    "import os, random\n",
    "import pandas as pd\n",
    "# import mir_utils as miru\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import librosa\n",
    "# import pytorch_utils\n",
    "# import pytorch_models\n",
    "#reload these libraries because I change them often-ish\n",
    "# imp.reload(pytorch_utils)\n",
    "# imp.reload(miru)\n",
    "# imp.reload(pytorch_models)\n",
    "from scipy.signal import resample\n",
    "from sklearn import preprocessing\n",
    "le_major = preprocessing.LabelEncoder()\n",
    "\n",
    "SR = 44100\n",
    "#functions\n",
    "spec = torchaudio.functional.spectrogram\n",
    "\n",
    "def getMeanLength(x):\n",
    "    gl=x.apply(lambda z: len(z[\"audio\"]),axis=1)\n",
    "    print(gl.mean()/SR,gl.mean(),x[\"label\"].iloc[0])\n",
    "\n",
    "audio_df = pd.read_csv(\"csvs/audio_df.csv\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_df[\"maj\"] = audio_df[\"maj\"].apply(lambda x: 1 if x==\"drums\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32 µs, sys: 14 µs, total: 46 µs\n",
      "Wall time: 49.8 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#define a dataset\n",
    "# reshample all sounds to 22050 and take the first half of a second (0:11025)\n",
    "class audioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,audio_df,SR=44100,transform=None):\n",
    "        self.audio_df = audio_df\n",
    "        self.minLength = SR//4\n",
    "    def __len__(self):\n",
    "        return len(self.audio_df)\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        row = self.audio_df.iloc[idx]\n",
    "        try:\n",
    "            signal,sr = librosa.load(row[\"path\"])\n",
    "        except:\n",
    "            signal = np.zeros(self.minLength)\n",
    "            sr = self.minLength\n",
    "        # resample to global SR\n",
    "        signal = librosa.resample(signal,sr,SR//2)\n",
    "        # pad the audio length if too short\n",
    "        nz = np.max((self.minLength-signal.shape[0],0))\n",
    "        signal = np.concatenate([signal[0:self.minLength],np.zeros(nz)])\n",
    "        \n",
    "        sound={\"signal\":signal,\"major\":row[\"maj\"],\"minor\":row[\"min\"],\"path\":row[\"path\"],\"sr\":SR}\n",
    "        return sound\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "adf = audio_df.copy().sample(frac=1)\n",
    "train,val = train_test_split(adf, test_size=0.1) \n",
    "\n",
    "train_loader = DataLoader(audioDataset(train,SR), batch_size=64,shuffle=False, num_workers=8)\n",
    "val_loader = DataLoader(audioDataset(val,SR), batch_size=128,shuffle=False, num_workers=8)\n",
    "\n",
    "sample_iterator = iter(train_loader)\n",
    "d = next(sample_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torchmetrics\n",
    "from torchmetrics.functional import auc\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint,LearningRateMonitor\n",
    "import torchaudio\n",
    "class Transformer_DVN(LightningModule):\n",
    "    def __init__(self,attention_dropout=0.3,d_model=100,heads=25,encoding_layers=16,pool_dim=2,pct_start=0.05,max_lr=1e-4,max_momentum=0.95,epochs = 50):\n",
    "        super().__init__()\n",
    "        dropout=0.2\n",
    "        self.attention_dropout=attention_dropout\n",
    "        self.d_model = d_model\n",
    "        self.heads = heads\n",
    "        self.encoding_layers = encoding_layers\n",
    "        self.pool_dim = pool_dim\n",
    "        self.pct_start = pct_start\n",
    "        self.max_lr = max_lr\n",
    "        self.max_momentum = max_momentum\n",
    "        self.epochs = epochs\n",
    "        self.spectrogram_func = torchaudio.transforms.Spectrogram(n_fft = int(self.d_model*2)-1, hop_length = 200, power = 0.2, normalized = True)\n",
    "        \n",
    "        self.pos_encoder = PositionalEncoding(self.d_model, 0.1)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=self.d_model, nhead=self.heads,\n",
    "                                                        dropout = self.attention_dropout,)\n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=self.encoding_layers)\n",
    "        self.adaptiveavgpool = nn.AdaptiveAvgPool1d(self.pool_dim)\n",
    "        self.adaptivemaxpool = nn.AdaptiveMaxPool1d(self.pool_dim)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "          nn.Linear(self.d_model*2*self.pool_dim,64),\n",
    "          nn.Linear(64,32),\n",
    "          nn.Linear(32,1),\n",
    "        )\n",
    "#         d[\"signal\"]\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x1 = self.spectrogram_func(x).transpose(0,1).transpose(0,2)\n",
    "        x1 = self.pos_encoder(x1)\n",
    "        x2 = self.transformer_encoder(x1).transpose(1,0).transpose(2,1)\n",
    "        x3r = self.adaptiveavgpool(x2)\n",
    "        x3c = self.adaptivemaxpool(x2)\n",
    "        x4 = torch.cat((x3r, x3c), dim=1)\n",
    "        x4 = x4.view(x4.size(0), -1)\n",
    "        out =  self.decoder(x4)\n",
    "        return out\n",
    "    \n",
    "    def step(self, batch, batch_idx):\n",
    "        x, y = batch[\"signal\"].float(),batch[\"major\"].float().reshape(-1,1)\n",
    "        x1 = self.spectrogram_func(x).transpose(0,1).transpose(0,2)\n",
    "        x1 = self.pos_encoder(x1)\n",
    "        x2 = self.transformer_encoder(x1).transpose(1,0).transpose(2,1)\n",
    "        x3r = self.adaptiveavgpool(x2)\n",
    "        x3c = self.adaptivemaxpool(x2)\n",
    "        x4 = torch.cat((x3r, x3c), dim=1)\n",
    "        x4 = x4.view(x4.size(0), -1)\n",
    "        out =  self.decoder(x4)\n",
    "#         loss = F.binary_cross_entropy_with_logits(out, y,pos_weight=self.w_pos.to(self.device))\n",
    "        loss = F.binary_cross_entropy_with_logits(out, y,)\n",
    "        accuracy = torchmetrics.functional.accuracy(out,y.int(),num_classes=1)\n",
    "        print(accuracy,end=\"\\r\")\n",
    "        return loss, {\"loss\": loss,\"accuracy\":accuracy}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, logs = self.step(batch, batch_idx)\n",
    "\n",
    "        \n",
    "        self.log_dict({f\"train_{k}\": v for k, v in logs.items()}, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, logs = self.step(batch, batch_idx)\n",
    "        self.log_dict({f\"val_{k}\": v for k, v in logs.items()}, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-8)\n",
    "        lr_scheduler = {'scheduler': torch.optim.lr_scheduler.OneCycleLR(\n",
    "                                        optimizer,\n",
    "                                        pct_start = self.pct_start,\n",
    "                                        max_lr=self.max_lr,\n",
    "                                        steps_per_epoch=int(len(self.train_dataloader())),\n",
    "                                        epochs=self.epochs,\n",
    "                                        anneal_strategy=\"cos\",\n",
    "                                        final_div_factor = 1000,\n",
    "                                        max_momentum=self.max_momentum,\n",
    "                                    ),\n",
    "                        'name': 'learning_rate',\n",
    "                        'interval':'step',\n",
    "                        'frequency': 1}\n",
    "        return [optimizer],[lr_scheduler]\n",
    "\n",
    "\n",
    "\n",
    "model = Transformer_DVN(attention_dropout=0.3,d_model=120,heads=30,encoding_layers=12,pool_dim=2,)\n",
    "\n",
    "model(d[\"signal\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback =  ModelCheckpoint(\n",
    "    monitor='val_accuracy',\n",
    "    dirpath='models/transformer',\n",
    "    filename='DVN{epoch:2d}-{val_accuracy:.3f}-{val_loss:.3f}',\n",
    "    save_top_k=3,\n",
    "    mode='max',\n",
    ")\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "trainer = pl.Trainer(gpus=1,precision=16,callbacks=[checkpoint_callback,lr_monitor],log_every_n_steps=1,max_epochs=100,stochastic_weight_avg=True,)\n",
    "trainer.fit(model,train_loader,val_loader,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE_FACTOR = 0.5\n",
    "num_windows = 5\n",
    "window_shift = 300\n",
    "smallest_loss,smallest_vloss = 1000,1000\n",
    "step = 0\n",
    "for epoch in range(40): \n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        train_loss = 0\n",
    "        bs = len(data[\"signal\"]) # batch size\n",
    "        signal = interpolate(data[\"signal\"].reshape([bs,1,-1]),scale_factor = SCALE_FACTOR,recompute_scale_factor=False).reshape([bs,1,-1])\n",
    "      \n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnet(signal,)\n",
    "        y = torch.tensor(le_major.transform(data[\"major\"])).to(device)\n",
    "        loss = loss_func(outputs,y.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        if i%5==0:\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0\n",
    "                for iv,datav in enumerate(test_loader, 0):\n",
    "                    signalv = interpolate(datav[\"signal\"].reshape([len(datav[\"signal\"]),1,-1]),scale_factor = SCALE_FACTOR,recompute_scale_factor=False).reshape([len(datav[\"signal\"]),-1])\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "                    voutputs = cnet(signalv,)\n",
    "                    vy = torch.tensor(le_major.transform(datav[\"major\"])).to(device)\n",
    "                    vloss = loss_func(voutputs,vy.long())\n",
    "                    val_loss += vloss\n",
    "            print('[%d, %d] val loss: %.5f, loss: %.5f'%(epoch + 1, i , val_loss,train_loss))\n",
    "    #         writer.add_scalar('Loss/Training', train_loss,)\n",
    "    #         writer.add_scalar('Loss/Validation', val_loss)\n",
    "            if val_loss < smallest_vloss:        \n",
    "                torch.save({\n",
    "                'epoch': epoch,\n",
    "                'vloss': vloss,\n",
    "                'model_state_dict': cnet.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                }, \"models/1d_conv//%.3f_%.4f_.checkpoint\"%(val_loss,train_loss,))\n",
    "                smallest_vloss = val_loss\n",
    "                smallest_loss = train_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
